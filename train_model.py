# %% [markdown]
# # Pipeline d'Entra√Ænement de Mod√®le K-Type
# 
# Ce notebook (script) impl√©mente un pipeline complet de Machine Learning pour la classification des fermes en Cas-Types (K-Types).
# Il couvre :
# 1.  Chargement et Pr√©paration des donn√©es
# 2.  Analyse Non Supervis√©e (Clustering K-Means) pour valider la structure des donn√©es
# 3.  Apprentissage Supervis√© (Random Forest vs XGBoost)
# 4.  Optimisation des Hyperparam√®tres (GridSearch)
# 5.  Export du meilleur mod√®le

# %%
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# %% [markdown]
# ## 1. Chargement et Pr√©paration des Donn√©es

# %%
# Chargement du dataset synth√©tique
df = pd.read_csv("farms_dataset.csv")
print(f"Dataset charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes")

# Nettoyage des donn√©es
df = df.dropna(subset=['ktype'])
print(f"Apr√®s suppression des NaNs K-Type : {df.shape[0]} lignes")

# Filtrage des classes trop rares pour la validation crois√©e
ktype_counts = df['ktype'].value_counts()
valid_ktypes = ktype_counts[ktype_counts >= 5].index
df = df[df['ktype'].isin(valid_ktypes)]
print(f"Apr√®s filtrage des K-Types rares (<5 samples) : {df.shape[0]} lignes, {len(valid_ktypes)} classes restantes")

print(df.head())

# S√©paration Features (X) et Target (y)
target_col = "ktype"
features_num = ["sau", "umo", "ugb", "nb_vl", "surface_sfp", "surface_herbe_pp", "surface_herbe_pt", "surface_culture"]
features_cat = ["region", "filiere"]

X = df[features_num + features_cat]
y = df[target_col]

# %% [markdown]
# ## 2. Analyse Non Supervis√©e (Clustering)
# Objectif : V√©rifier si des groupes naturels √©mergent des donn√©es sans utiliser les √©tiquettes K-Type.
# Cela permet de valider la pertinence de nos segments.

# %%
# Preprocessing pour le clustering (uniquement num√©rique pour simplifier ici)
scaler_clust = StandardScaler()
X_clust = scaler_clust.fit_transform(df[features_num])

# K-Means avec k=7 (approx nombre de fili√®res)
kmeans = KMeans(n_clusters=7, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_clust)

# Ajout des clusters au dataframe pour analyse
df["cluster"] = clusters

# Visualisation (Croisement Cluster vs Vrai K-Type)
crosstab = pd.crosstab(df["cluster"], df["ktype"])
print("\n--- Correspondance Clusters (Non Supervis√©) vs K-Types (Supervis√©) ---")
print(crosstab)
# Si la diagonale (ou des blocs) est forte, cela valide notre segmentation.

# %% [markdown]
# ## 3. Apprentissage Supervis√© & Optimisation
# Nous allons tester plusieurs mod√®les et optimiser leurs hyperparam√®tres.

# %%
# Pr√©paration du Pipeline de Preprocessing
# - Variables num√©riques : Standardisation (Centrer-R√©duire)
# - Variables cat√©gorielles : OneHotEncoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), features_num),
        ('cat', OneHotEncoder(handle_unknown='ignore'), features_cat)
    ])

# Split Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# D√©finition des mod√®les √† tester
model_pipelines = {
    'RandomForest': Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42, n_jobs=1))
    ]),
    'GradientBoosting': Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', GradientBoostingClassifier(random_state=42))
    ])
}

# Grilles d'hyperparam√®tres simplifi√©es pour rapidit√©
param_grids = {
    'RandomForest': {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [None, 15]
    },
    'GradientBoosting': {
        'classifier__n_estimators': [100],
        'classifier__learning_rate': [0.1],
        'classifier__max_depth': [3, 5]
    }
}

# %% [markdown]
# ## 4. Entra√Ænement et S√©lection du Meilleur Mod√®le

# %%
best_model = None
best_score = 0
best_name = ""

results = {}

print("\n--- D√©but de l'optimisation (GridSearch) ---")

for name, pipeline in model_pipelines.items():
    print(f"Optimisation de {name}...")
    # n_jobs=1 pour √©viter les probl√®mes d'interruption en environnement restreint
    grid_search = GridSearchCV(pipeline, param_grids[name], cv=3, n_jobs=1, verbose=2)
    grid_search.fit(X_train, y_train)
    
    score = grid_search.best_score_
    print(f"  > Meilleur Score CV : {score:.4f}")
    print(f"  > Meilleurs Params : {grid_search.best_params_}")
    
    results[name] = grid_search
    
    if score > best_score:
        best_score = score
        best_model = grid_search.best_estimator_
        best_name = name

print(f"\nüèÜ MEILLEUR MOD√àLE : {best_name} avec une pr√©cision CV de {best_score:.4f}")

# %% [markdown]
# ## 5. √âvaluation Finale sur le Test Set

# %%
y_pred = best_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

print("\n--- Rapport de Classification sur Test Set ---")
print(f"Pr√©cision Globale : {acc:.4f}")
print(classification_report(y_test, y_pred))

# %% [markdown]
# ## 6. Sauvegarde du Mod√®le
# Le pipeline complet (Preprocessing + Mod√®le optimis√©) est sauvegard√©.

# %%
joblib.dump(best_model, "model_ktype.pkl")
print("\n‚úÖ Mod√®le sauvegard√© sous 'model_ktype.pkl'")

# Test de rechargement rapide
loaded_model = joblib.load("model_ktype.pkl")
sample = X_test.iloc[0:1]
print("\nTest de pr√©diction sur un exemple :")
print(sample)
print(f"Pr√©diction : {loaded_model.predict(sample)[0]}")
